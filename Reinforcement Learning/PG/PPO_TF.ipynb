{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy      as np\n",
    "import tensorflow as tf\n",
    "from tqdm import tnrange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Params & Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env     = gym.make('CartPole-v0')\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "act_dim = env.action_space.n\n",
    "\n",
    "seed = 42\n",
    "tf.set_random_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "pi_lr           = 3e-4\n",
    "v_lr            = 1e-3\n",
    "clip_ratio      = 0.2\n",
    "max_kl          = .01\n",
    "epochs          = 10\n",
    "steps_per_epoch = 1000\n",
    "render          = False\n",
    "train_pi_iters  = 80\n",
    "train_v_iters   = 8\n",
    "hidden_sizes    = [64,64]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0,
     12
    ]
   },
   "outputs": [],
   "source": [
    "def discount_cumsum(rewards, discount):\n",
    "    '''\n",
    "    Method generates a discounted cumulative sum from a reward trajectory\n",
    "    [1,2,3] -> [3*d + 2*d + 1*d, 2*d + 1*d, 1*d]\n",
    "    '''\n",
    "    result = []\n",
    "    for i in range(len(rewards)):\n",
    "        d_cummulative_sum = sum([discount * x for x in rewards[i:]])\n",
    "        result.append(d_cummulative_sum)\n",
    "\n",
    "    return result\n",
    "\n",
    "def mlp(x, hidden_sizes, activation=tf.nn.relu, output_activation=None):\n",
    "    '''\n",
    "    Cleanest dynamic generation of dense layered NN I've ever seen\n",
    "    '''\n",
    "    for h in hidden_sizes[:-1]:\n",
    "        x = tf.layers.dense(x, units=h, activation=activation)\n",
    "    return tf.layers.dense(x, units=hidden_sizes[-1], activation=output_activation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class PPOBuffer:\n",
    "    \n",
    "    def __init__(self, obs_dim, gamma=0.99, lam=0.97):\n",
    "        \n",
    "        self.obs_dim   = obs_dim\n",
    "\n",
    "        self.obs_buf   = np.empty((0, self.obs_dim))\n",
    "        self.act_buf   = np.array([])\n",
    "        self.rew_buf   = np.array([])\n",
    "        self.val_buf   = np.array([])\n",
    "        self.adv_buf   = np.array([])\n",
    "        self.ret_buf   = np.array([])\n",
    "        self.logp_buf  = np.array([])\n",
    "\n",
    "        self.gamma     = gamma\n",
    "        self.lam       = lam\n",
    "        self.idx_s     = 0\n",
    "        self.idx_e     = 0\n",
    "    \n",
    "    \n",
    "    def store(self, obs, act, rew, val, logp):\n",
    "        \n",
    "        self.obs_buf  = np.append(self.obs_buf,  obs, axis=0)\n",
    "        self.act_buf  = np.append(self.act_buf,  act)\n",
    "        self.rew_buf  = np.append(self.rew_buf,  rew)\n",
    "        self.val_buf  = np.append(self.val_buf,  val)\n",
    "        self.logp_buf = np.append(self.logp_buf, logp)\n",
    "\n",
    "        self.idx_e += 1\n",
    "    \n",
    "    \n",
    "    def finish_path(self, last_val=0):\n",
    "\n",
    "        path_slice = slice(self.idx_s, self.idx_e)\n",
    "        rews = np.append(self.rew_buf[path_slice], last_val)\n",
    "        vals = np.append(self.val_buf[path_slice], last_val)\n",
    "\n",
    "        # the next two lines implement GAE-Lambda advantage calculation\n",
    "        deltas = rews[:-1] + self.gamma * vals[1:] - vals[:-1]\n",
    "        self.adv_buf = np.append(self.adv_buf, discount_cumsum(deltas, self.gamma * self.lam))\n",
    "\n",
    "        # the next line computes rewards-to-go, to be targets for the value function\n",
    "        self.ret_buf = np.append(self.ret_buf, discount_cumsum(rews, self.gamma)[:-1])\n",
    "\n",
    "        self.idx_s = self.idx_e\n",
    "        \n",
    "    def get(self):\n",
    "        # Normalize the advantages\n",
    "        self.adv_buf = (self.adv_buf - np.mean(self.adv_buf)) / np.std(self.adv_buf)\n",
    "\n",
    "        return [self.obs_buf, self.act_buf, self.adv_buf, self.ret_buf, self.logp_buf]\n",
    "    \n",
    "    def reset(self):\n",
    "        \n",
    "        self.obs_buf  = np.empty((0, self.obs_dim))\n",
    "        self.act_buf  = np.array([])\n",
    "        self.rew_buf  = np.array([])\n",
    "        self.val_buf  = np.array([])\n",
    "        self.adv_buf  = np.array([])\n",
    "        self.ret_buf  = np.array([])\n",
    "        self.logp_buf = np.array([])\n",
    "        self.idx_e    = 0\n",
    "        self.idx_s    = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Place Holders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_ph      = tf.placeholder(tf.float32, shape=(None, obs_dim))\n",
    "act_ph      = tf.placeholder(tf.int32,   shape=(None,))\n",
    "rew_ph      = tf.placeholder(tf.float32, shape=(None,))\n",
    "adv_ph      = tf.placeholder(tf.float32, shape=(None,))\n",
    "ret_ph      = tf.placeholder(tf.float32, shape=(None,))\n",
    "logp_old_ph = tf.placeholder(tf.float32, shape=(None,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     1,
     17
    ]
   },
   "outputs": [],
   "source": [
    "# Actor\n",
    "with tf.variable_scope('pi'):\n",
    "    logits   = mlp(obs_ph, hidden_sizes=hidden_sizes+[act_dim], activation=tf.nn.relu, output_activation=None)\n",
    "    pi       = tf.squeeze(tf.multinomial(logits, num_samples=1), axis=1) # Our Action\n",
    "    logp     = tf.reduce_sum(tf.one_hot(act_ph, depth=act_dim) * tf.nn.log_softmax(logits), axis=1) # Previous Actions\n",
    "    logp_pi  = tf.reduce_sum(tf.one_hot(pi,     depth=act_dim) * tf.nn.log_softmax(logits), axis=1) # Hypothetical Actions\n",
    "\n",
    "    # PPO objectives\n",
    "    ratio       = tf.exp(logp - logp_old_ph)\n",
    "    clipped_adv = tf.where(adv_ph > 0, (1 + clip_ratio) * adv_ph, (1 - clip_ratio) * adv_ph)\n",
    "    pi_loss     = -tf.reduce_mean(tf.minimum(ratio * adv_ph, clipped_adv))\n",
    "    train_pi    = tf.train.AdamOptimizer(learning_rate=pi_lr).minimize(pi_loss)\n",
    "\n",
    "    # PPO Used for early stopping during training\n",
    "    approx_kl = tf.abs(tf.reduce_mean(logp_old_ph - logp))\n",
    "\n",
    "# Critic\n",
    "with tf.variable_scope('v'):\n",
    "    v = tf.squeeze(mlp(obs_ph, hidden_sizes=hidden_sizes+[1], activation=tf.tanh, output_activation=None), axis=1)\n",
    "    v_loss  = tf.reduce_mean((ret_ph - v) ** 2)\n",
    "    train_v = tf.train.AdamOptimizer(learning_rate=v_lr).minimize(v_loss)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def update(feed_ph, sess, max_kl=0.01, train_pi_iters=80, train_v_iters=80):\n",
    "    inputs = {k:v for k,v in zip(feed_ph, buf.get())}\n",
    "    pls, vls = [], []\n",
    "    \n",
    "    # Policy gradient step\n",
    "    for i in range(train_pi_iters):\n",
    "        _, pl, kl = sess.run([train_pi, pi_loss, approx_kl], feed_dict=inputs)\n",
    "        pls.append(pl)\n",
    "        \n",
    "        # Early stopping\n",
    "        if kl > 1.5 * max_kl:\n",
    "            print('Early stopping at step %d due to reaching max kl.'%i)\n",
    "            break\n",
    "\n",
    "    # Value function learning\n",
    "    for _ in range(train_v_iters):\n",
    "        _,vl = sess.run([train_v, v_loss], feed_dict=inputs)\n",
    "        vls.append(vl)\n",
    "    \n",
    "    return [pls, vls]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c55298cf5aa424a924bb46a465201b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, rewards: 25.00,               vl mean: 430.1942, pl mean: -0.0111\n",
      "Epoch 2, rewards: 33.33,               vl mean: 829.2031, pl mean: -0.0167\n",
      "Early stopping at step 12 due to reaching max kl.\n",
      "Epoch 3, rewards: 52.63,               vl mean: 2387.7151, pl mean: -0.0061\n",
      "Epoch 4, rewards: 76.92,               vl mean: 3155.3511, pl mean: -0.0126\n",
      "Epoch 5, rewards: 90.91,               vl mean: 6817.0640, pl mean: -0.0090\n",
      "Epoch 6, rewards: 142.86,               vl mean: 8485.3613, pl mean: -0.0094\n",
      "Epoch 7, rewards: 111.11,               vl mean: 7352.4951, pl mean: -0.0109\n",
      "Epoch 8, rewards: 111.11,               vl mean: 7090.7715, pl mean: -0.0107\n",
      "Epoch 9, rewards: 142.86,               vl mean: 9058.7588, pl mean: -0.0094\n",
      "Epoch 10, rewards: 90.91,               vl mean: 7792.9321, pl mean: -0.0104\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s = tf.InteractiveSession()\n",
    "s.run(tf.global_variables_initializer())\n",
    "\n",
    "tb = tf.summary.FileWriter( './logs/1/train ', s.graph)\n",
    "\n",
    "buf = PPOBuffer(obs_dim)\n",
    "bar = tnrange(epochs)\n",
    "for epoch in bar:\n",
    "\n",
    "    obs, rew, done = env.reset(), 0, False\n",
    "    total_episode_reward, epoch_rews = 0, []\n",
    "    finished_rendering_this_epoch    = False\n",
    "\n",
    "    # -------- Start Batch -------- #\n",
    "    for step in range(steps_per_epoch):\n",
    "        \n",
    "        if render and not finished_rendering_this_epoch:\n",
    "            env.render()\n",
    "\n",
    "        act, v_t, logp_t = s.run([pi, v, logp_pi], feed_dict={obs_ph: obs[None,:]})\n",
    "        buf.store([obs], act, rew, v_t, logp_t)\n",
    "\n",
    "        obs, rew, done, _ = env.step(act[0])\n",
    "        total_episode_reward += rew\n",
    "\n",
    "        if done or (step==steps_per_epoch-1):\n",
    "            last_val = rew if done else s.run(v, feed_dict={obs_ph: obs[None,:]})\n",
    "            buf.finish_path(last_val)\n",
    "\n",
    "            epoch_rews.append(total_episode_reward)\n",
    "            obs, rew, done       = env.reset(), 0, False\n",
    "            total_episode_reward =0\n",
    "\n",
    "            finished_rendering_this_epoch = True\n",
    "\n",
    "    pls, vls = update([obs_ph, act_ph, adv_ph, ret_ph, logp_old_ph], s, max_kl=max_kl, train_pi_iters=train_pi_iters, train_v_iters=train_v_iters)\n",
    "    bar.write(f'Epoch {epoch+1}, rewards: {(sum(epoch_rews)/len(epoch_rews)):.2f}, \\\n",
    "              vl mean: {np.mean(vls):.4f}, pl mean: {np.mean(pls):.4f}')\n",
    "    buf.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scratch paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'pi/dense/kernel:0' shape=(4, 64) dtype=float32_ref>\n",
      "(4, 64)\n",
      "----\n",
      "<tf.Variable 'pi/dense/bias:0' shape=(64,) dtype=float32_ref>\n",
      "(64,)\n",
      "----\n",
      "<tf.Variable 'pi/dense_1/kernel:0' shape=(64, 64) dtype=float32_ref>\n",
      "(64, 64)\n",
      "----\n",
      "<tf.Variable 'pi/dense_1/bias:0' shape=(64,) dtype=float32_ref>\n",
      "(64,)\n",
      "----\n",
      "<tf.Variable 'pi/dense_2/kernel:0' shape=(64, 2) dtype=float32_ref>\n",
      "(64, 2)\n",
      "----\n",
      "<tf.Variable 'pi/dense_2/bias:0' shape=(2,) dtype=float32_ref>\n",
      "(2,)\n",
      "----\n",
      "<tf.Variable 'v/dense/kernel:0' shape=(4, 64) dtype=float32_ref>\n",
      "(4, 64)\n",
      "----\n",
      "<tf.Variable 'v/dense/bias:0' shape=(64,) dtype=float32_ref>\n",
      "(64,)\n",
      "----\n",
      "<tf.Variable 'v/dense_1/kernel:0' shape=(64, 64) dtype=float32_ref>\n",
      "(64, 64)\n",
      "----\n",
      "<tf.Variable 'v/dense_1/bias:0' shape=(64,) dtype=float32_ref>\n",
      "(64,)\n",
      "----\n",
      "<tf.Variable 'v/dense_2/kernel:0' shape=(64, 1) dtype=float32_ref>\n",
      "(64, 1)\n",
      "----\n",
      "<tf.Variable 'v/dense_2/bias:0' shape=(1,) dtype=float32_ref>\n",
      "(1,)\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "for v in tf.trainable_variables():\n",
    "    n = np.array(v.eval())\n",
    "    print(v)\n",
    "    print(n.shape)\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
